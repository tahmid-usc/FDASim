---
title: "Investigating Mean Integrated Squared Error calculation in R"
author: "Tahmidul Islam"
date: "7/1/2020"
output: 
 html_document:
  fig_width: 14 
  fig_height: 6
---

```{r setup, include = FALSE, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = F, message = F, warning = F)
```

```{r}
#loading packages
rm(list = ls())
library(MASS)
library(kernlab)
library(mvtnorm)
library(Matrix)
library(optimx)
library(fdapace)
library(dplyr)
library(cubature)
library(smoothmest)
library(kableExtra)

source('../code/RBF.R')
source('../code/functionsNew.R')
#source('code/laplace.R')
#source('code/matern52.R')
#source('code/matern32.R')
source('../code/multistart.R')
```

We propose two new mean functions to be used for functional data simulation. One is based on normal density functions with different location and scale parameters. The other is based on Laplace (double exponential) distribution with similar manipulation. These two mean functions contain spikes of several degrees (depending on the scale parameter). The Laplace distribution outputs non-differentiable regions at the spikes.
Proposed new mean function:
$$
\mu_1(t) = .25 \ N(-.95, .06) + .25 \ N(-.5, .05) + .25 \ N(0, .03) + .25 \ N(.5, .02)    
$$
$$
\mu_2(t) = .25 \ dexp(-.95, .06) + .25 \ dexp(-.5, .05) + .25 \ dexp(0, .03) + .25 \ dexp(.5, .02)    
$$
Where $N(a,b)$ and $dexp(a,b)$ are the density function of a normal distribution and a Laplace distribution respectively with mean $a$ and standard deviation $b$.

Let us plot the mean funtions.

```{r}
t <- seq(-1, 1, length.out = 100)
par(mfrow = c(1,2))
curve(muf1, -1, 1, lwd = 2, main = 'Mix of normal')
curve(muf2, -1, 1, lwd = 2, main = 'Mix of double exponential')
```

Now simulate some random functions using the mean functions as the prior mean in the Gaussian process with Gaussian covariance function.

```{r}

func <- funcgen(muf1, theta = c(.01,.1))

par(mfrow = c(1,2))
plot(func$x, func$y, type = 'l', lwd = 3, 
     main = 'Mix of normal', xlab = 'Time', ylab = 'Y',
     cex.main = 1.5, cex.lab = 1.5, ylim=c(-2, 2))
for(i in 1:10) {
  func <- funcgen(muf1, theta = c(.1,.1))
  lines(func$x, func$y, lwd = 3)
}

func <- funcgen(muf2, theta = c(.01,.1))
plot(func$x, func$y, type = 'l', lwd = 3, 
     main = 'Mix of double exponential', xlab = 'Time', ylab = 'Y',
     cex.main = 1.5, cex.lab = 1.5, ylim=c(-2, 2))
for(i in 1:10) {
  func <- funcgen(muf2, theta = c(.1,.1))
  lines(func$x, func$y, lwd = 3)
}

```


Now we generate sparse functional data which are actually observed in practice. To introduce sparsity, first we sample any integer $n_t$ between 2 to 10. This is the number of time points each function will be observed on. Further we sample $n_t$ time points from a uniform (-1, 1). 

```{r}
fdata <- fdagen(n = 20, maxt = 10, muf = muf1, theta = c(.01,.1,.01))

par(mfrow = c(1,2))

plot(1, type="n", xlim=c(-1, 1), ylim=c(-2, 2), 
     main = 'Mix of normal', xlab = 'Time', ylab = 'Y',
     cex.main = 1.5, cex.lab = 1.5)
for(i in 1:20){
  lines(fdata[fdata$id == i,1], fdata[fdata$id == i,2], type = 'b', lwd = 2, col = rgb(0,0,0,.5))
}
lines(t, muf1(t), type = 'l', lwd = 4, ylim = c(-3,3))

fdata <- fdagen(n = 20, maxt = 10, muf = muf2, theta = c(.01,.1,.01))

plot(1, type="n", xlim=c(-1, 1), ylim=c(-2, 2), 
     main = 'Mix of double exponential', xlab = 'Time', ylab = 'Y',
     cex.main = 1.5, cex.lab = 1.5)
for(i in 1:20){
  lines(fdata[fdata$id == i,1], fdata[fdata$id == i,2], type = 'b', lwd = 2, col = rgb(0,0,0,.5))
}
lines(t, muf2(t), type = 'l', lwd = 4, ylim = c(-3,3))

```

We will fit our GP based model to the generated data and compare with the fit with the PACE (Yao, Mueller, and Wang (2005)) method based on Functional Principal Component Analysis (FPCA), a key technique for functional data analysis, for sparsely or densely sampled random trajectories and time courses, via the Principal Analysis by Conditional Estimation (PACE) algorithm.

## Mixed of normal density mean function

```{r}
# generate data from mixed normal
source('../code/RBF.R')
fdata1 <- fdagen(n = 20, maxt = 10, muf = muf1, theta = c(.01,.1,.01))
time <- seq(-1, 1, length.out = 100)
```

```{r, results = 'hide'}
# GP fit with all available kernels

#Gaussian kernel
source('../code/RBF.R')
fet.muf1.rbf <- feature(fdata1)
posMean.muf1.rbf <- gpsmooth(time, fet.muf1.rbf)

#Laplace kernel

source('../code/laplace.R')
fet.muf1.lap <- feature(fdata1)
posMean.muf1.lap <- gpsmooth(time, fet.muf1.lap)

# Mtern 5/2 kernel

source('../code/matern52.R')
fet.muf1.m52 <- feature(fdata1)
posMean.muf1.m52 <- gpsmooth(time, fet.muf1.m52)

# Matern 3/2

source('../code/matern32.R')
fet.muf1.m32 <- feature(fdata1)
posMean.muf1.m32 <- gpsmooth(time, fet.muf1.m32)
```

```{r, results = 'hide'}
# PACE
fpca.dt1 <- MakeFPCAInputs(IDs = fdata1$id, fdata1$x, fdata1$y, sort = FALSE)
fpca1 <- FPCA(fpca.dt1$Ly, fpca.dt1$Lt, optns = list(dataType = 'Sparse'))

fdata1 <- arrange(fdata1, x)
muf1.fpca <- Lwls1D(fpca1$bwMu, xin = fdata1$x, yin = fdata1$y, xout = time, kernel_type = 'gauss')

fmu1 <- fpcamu(time, fpca1)
```

We can inspect the fit of the mean function estimation by GP based method and compare with PACE. For GP method, we have used four different covariance kernels: Gaussian (RBF), Laplace, Matern 5/2 and Matern 3/2.

```{r}
plot(time, muf1.fpca, type = 'l', lwd = 3, ylim = c(-1,2), xlab = 'Time', ylab = 'Y', col = 6, main = 'Mix of normal densities')
points(fdata1$x, fdata1$y, pch = 16, cex = .8)
lines(time, posMean.muf1.rbf, type = 'l', lwd = 3, col = 2)
lines(time, posMean.muf1.lap, type = 'l', lwd = 3, col = 3)
lines(time, posMean.muf1.m52, type = 'l', lwd = 3, col = 4)
lines(time, posMean.muf1.m32, type = 'l', lwd = 3, col = 5)
lines(time, muf1(time), lwd = 3, col = 1)
legend("top", c("True", "Gaussian", 'Laplace', 'Matern 5/2', 'Matern 3/2', "PACE"), col = 1:6, lty = 1, bty = 'n', horiz = T, lwd = 3)
```


## Error in mean function estimation

### Mean Squared Error (MSE)

We want to measure the goodness of fit of our GP based method and compare the performance with PACE. The basic error metric we can use for mean function estimation is the Mean Squared Error (MSE). Since we know the true mean function, we can compute the following:
$$
\text{MSE} = \frac{1}{n} \sum_{i = 1}^N \Big( \hat \mu(t_i) - \mu(t_i) \Big)^2.
$$
Here the time points $t_i$'s form a fixed grid of points over the support of the function. Here we can choose $N = 100, 500, 1000, \dots$ time points from the interval $(-1,1)$ and compute the MSE. We want to investigate the effect of the choice of $N$. 

### Mix of normal densities

```{r}
n <- c(10, 50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000, 50000)
kername <- c('rbf', 'lap', 'm52', 'm32')
mse.muf1 <- c()
for(i in kername) {
  mse.muf1 <- cbind(mse.muf1, vrmse(n, method = 'unif', muf = muf1, est = gpsmooth, est_arg = get(paste0('fet.muf1.', i))))
}
mse.muf1 <- cbind(mse.muf1, vrmse(n, method = 'unif', muf = muf1, est = fpcamu, est_arg = fpca1))
mse.muf1 <- as.data.frame(mse.muf1)
```

```{r}
names(mse.muf1) <- c('Gaussian', 'Laplace', 'Matern_5/2', 'Matern 3/2', 'PACE')
mse.muf1 <-cbind(n, mse.muf1)
mse.muf1 %>% kable() %>% kable_styling()
```

```{r}
plot(n, mse.muf1[,2], col = 1, pch = 16, ylim = c(min(mse.muf1[,-1]), max(mse.muf1[,-1])), type = 'l', 
     xlab = 'Grid size', ylab = 'MSE', lwd = 3, main = 'Fixed grid')
for (i in 3:6) {
  lines(n, mse.muf1[,i], col = i-1, pch = 16, lwd = 3)
}
legend('bottom', c('Gaussian', 'Laplace', 'Matern_5/2', 'Matern 3/2', 'PACE'), lty = 1, col = 1:5, bty = 'n', horiz = T)
```

